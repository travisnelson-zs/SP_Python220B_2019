Initial Baseline Profile Using 'gtime' (truncated to useful time data only):
------------------------------------------------------------------------------------
Danielles-MBP:lesson06 danicoates$ gtime --verbose python poor_perf.py 
{'2013': 101011, '2014': 101256, '2015': 101998, '2016': 101347, '2017': 203970, '2018': 0}
'ao' was found 499836 times
	Command being timed: "python poor_perf.py"
	User time (seconds): 5.22
	System time (seconds): 0.19
	Percent of CPU this job got: 99%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:05.44
	
Initial Baseline Profile Usoing cProfile (truncated to useful time data only):
------------------------------------------------------------------------------------
Danielles-MBP:lesson06 danicoates$ python -m cProfile -s time poor_perf.py 
{'2013': 101011, '2014': 101256, '2015': 101998, '2016': 101347, '2017': 203970, '2018': 0}
'ao' was found 499836 times
         1039473 function calls (1039456 primitive calls) in 5.578 seconds

   Ordered by: internal time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    5.332    5.332    5.472    5.472 poor_perf.py:9(analyze)
        1    0.098    0.098    5.570    5.570 poor_perf.py:59(main)
  1000022    0.083    0.000    0.083    0.000 {method 'append' of 'list' objects}
    18938    0.032    0.000    0.032    0.000 {built-in method _codecs.utf_8_decode}
    18938    0.024    0.000    0.056    0.000 codecs.py:319(decode)
        3    0.003    0.001    0.003    0.001 {built-in method _imp.create_dynamic}
        2    0.001    0.001    0.001    0.001 {method 'read' of '_io.FileIO' objects}
        2    0.001    0.000    0.001    0.000 {built-in method marshal.loads}
        2    0.001    0.000    0.002    0.001 <frozen importlib._bootstrap_external>:914(get_data)

Initial Times for Specific Code Blocks
------------------------------------------------------------------------------------		
Time to process first CSV run through using timeit.timer:  3.331484221
Time to process second CSV run throughusing timeit.timer:  1.8776932019999997
Time to run first row-by-row read of CSV using timeit.timer:  2.327549435
Time to run first 'for' loop using timeit.timer:  1.0333610270000002

Total Time Reducing CSV to Single Read = Reduced time by 1.78 seconds
------------------------------------------------------------------------------------
Danielles-MBP:lesson06 danicoates$ gtime --verbose python good_perf.py 
{'2013': 101011, '2014': 101256, '2015': 101998, '2016': 101347, '2017': 203970, '2018': 0}
'ao' was found 499836 times
	Command being timed: "python good_perf.py"
	User time (seconds): 3.50
	System time (seconds): 0.15
	Percent of CPU this job got: 99%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:03.66

Remove 'for' Loop and 'append', and Update Counts as the CSV is Parsed = Reduced time by .81 seconds
------------------------------------------------------------------------------------
Danielles-MBP:lesson06 danicoates$ gtime --verbose python good_perf.py 
{'2013': 101011, '2014': 101256, '2015': 101998, '2016': 101347, '2017': 102121, '2018': 101849}
'ao' was found 499836 times
	Command being timed: "python good_perf.py"
	User time (seconds): 2.80
	System time (seconds): 0.04
	Percent of CPU this job got: 99%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:02.85
	
SUMMARY
------------------------------------------------------------------------------------
1.  Cleaned up code to satisfy pylint
2.  Simplified code to open CSV file once
3.  Removed list that was being appended as it isn't returned
4.  Removed 'for' loop and instead incremented counters during row read
5.  Overall reduction in time by 2.42 seconds, which is ~50% faster
	
