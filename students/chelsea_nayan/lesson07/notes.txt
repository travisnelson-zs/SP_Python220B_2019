----------Findings on the import logic from lesson05-----------------

First I expanded the customers.csv, products.csv, and rentals.csv so they would show 1000 entries each using db_expander.py.

Then I made it so that all three csv files would be imported separately (in linear.py), timing how long it took to import products.csv and customers.csv to MongoDB (as well as the total time for the program to run). 

I ran the program and found that it took 2.269 seconds to import products.csv, 2.255 seconds to import customers.csv, and 2.357 seconds to import rentals.csv. Overall, it took ~7.375 seconds to run linear.py. 

The amended import logic (seen in parallel.py) was written to import customers, products, and rentals onto MondoDB on separate threads. This time it took 2.818 seconds to import products.csv, 3.912 seconds to import customers.csv, and 3.438 seconds to import rentals.csv. The total time it took to run parallel.py was ~10.168 seconds. 

The time it took to run parallel.py compared to linear.py was increased by 2.793 seconds which is an increase of 37.87%. 

I figured running in parallel would have been faster, so I was surprised. The second run through I got 6.907 seconds for parallel.py, then 6.338 seconds, then 8.081 seconds. I ran linear.py again and then got 7.883 seconds, 6.760 seconds, and then 9.862 seconds. 

It seems that both modules (parallel.py and linear.py) are similar in their runtimes.

A point of contention could possibly be when one thread is waiting for an object that is currently being held ny another thread. So if you lock the object in the queue when another thread is expecting it, then the for loop that runs the threads will be running forever. In parallel.py if you take away the output_queue.put(...), then the for loop in main will never stop.

----------Findings after revisions---------

After launching a separate join in a different loop to make sure all threads finish (and removing storage_queue()), parallel.py seemed to take 6.301 seconds to run. Adding the different entries for each of the three csv files happened 'at the same time.' It took linear.py 6.784 seconds to run. this was about an 8% decrease in time. Running concurrently seems to be faster than running linearly, but not by much (at least by my oldie laptop) but it would still be worth it. 

Contention could be caused if all the threads were writing to the same database. In parallel.py, there was a separate database I was writing to with each thread. 