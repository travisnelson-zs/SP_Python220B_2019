The linear and parallel run times listed below are for three files with 1000 records in each. The times varied between runs but the totals below are about average.
Depending upon how you look at the numbers either option could be faster. If you look at the aggregate run time for each file linear.py does better than parallel.py but 
cProfile shows a definite advantage to the parallel option. 

While contention is possible MongoDB and both programs are built to avoid contention. The count and time values for each file are set for each specific collection and do not depend on the values in the other collections. 
The MongoDb client could have contention but it is thread safe. Lastly, there could be contention when the data for the rental file is added. The rental file gets customer id and product id from the first two files 
but this does not appear to be an issue in MongoDb right now.  

cProfile shows better times for the parallel program, but there is not a noticiable difference between the two options with the current number of customers. I recommend management move forward with the project only 
if we expect the number of customers to increase dramatically in the future. The small difference is not noticable with 1000 customers but would be noticable with 1 million customers.  

Linear Run Time:
1000 0 1000 product 0.046989
1000 0 1000 customer 0.056951
1000 0 1000 rental 0.062263
main run time:      0.213177
aggregate run time: 0.166203
[(1000, 0, 1000, 0.213177), (1000, 0, 1000, 0.213177), (1000, 0, 1000, 0.213177)]
         74405 function calls (74267 primitive calls) in 0.223 seconds

Parallel Run Time:
1000 0 1000 customer 0.051964
1000 0 1000 product 0.118908
1000 0 1000 rental 0.135909
main run time:      0.181876
aggregate run time: 0.30678099999999997
[(1000, 0, 1000, 0.181876), (1000, 0, 1000, 0.181876), (1000, 0, 1000, 0.181876)]
         3462 function calls (3389 primitive calls) in 0.192 seconds

